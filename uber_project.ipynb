{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a03536",
   "metadata": {},
   "source": [
    "**Project ðŸš§**\n",
    "\n",
    "One of the main pain point that Uber's team found is that sometimes drivers are not around when users need them. For example, a user might be in San Francisco's Financial District whereas Uber drivers are looking for customers in Castro.  \n",
    "\n",
    "(If you are not familiar with the bay area, check out <a href=\"https://www.google.com/maps/place/San+Francisco,+CA,+USA/@37.7515389,-122.4567213,13.43z/data=!4m5!3m4!1s0x80859a6d00690021:0x4a501367f076adff!8m2!3d37.7749295!4d-122.4194155\" target=\"_blank\">Google Maps</a>)\n",
    "\n",
    "Eventhough both neighborhood are not that far away, users would still have to wait 10 to 15 minutes before being picked-up, which is too long. Uber's research shows that users accept to wait 5-7 minutes, otherwise they would cancel their ride. \n",
    "\n",
    "Therefore, Uber's data team would like to work on a project where **their app would recommend hot-zones in major cities to be in at any given time of day.**\n",
    "\n",
    "**Goals ðŸŽ¯**\n",
    "\n",
    "Uber already has data about pickups in major cities. Your objective is to create algorithms that will determine where are the hot-zones that drivers should be in. Therefore you will:\n",
    "\n",
    "* Create an algorithm to find hot zones \n",
    "* Visualize results on a nice dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5856710e",
   "metadata": {},
   "source": [
    "# Basic information\n",
    "\n",
    "Let's import our libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d58e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97399310",
   "metadata": {},
   "source": [
    "Our data is fragmented. Let's work a little on it to bring it up together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a31637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(filename):\n",
    "    \"\"\"Prepares batch extraction of our fragmented csv files\"\"\"\n",
    "    name_without_ext = filename.replace('.csv', '')\n",
    "    name = name_without_ext.split('-')[-1]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f2ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_dict = {}\n",
    "\n",
    "for file in os.listdir('../uber-trip-data'):\n",
    "    if file.endswith('.csv'):\n",
    "        key = get_name(file)\n",
    "        uber_dict[key] = pd.read_csv(f'../uber-trip-data/{file}')\n",
    "        print(f\"AjoutÃ©: {key}\")\n",
    "    else:\n",
    "        print(f'{file} is not a csv!')\n",
    "\n",
    "print(f\"\\nLoaded CSV files: {len(uber_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47056496",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avsep14 = pd.concat([uber_dict['apr14'], uber_dict['may14'], uber_dict['jun14'],\n",
    "                        uber_dict['jul14'], uber_dict['aug14'], uber_dict['sep14']], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avsep14.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d7bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avsep14.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0ab0e",
   "metadata": {},
   "source": [
    "Our data for 2014 has been gathered into a single dataframe with more than 4.5 million entries. Now for 2015:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6ef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jajun15 = pd.read_csv(f'../uber-trip-data/raw-data-15/uber-raw-data-janjune-15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jajun15.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721efcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jajun15.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaf66f7",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa40cf24",
   "metadata": {},
   "source": [
    "### 2014\n",
    "\n",
    "First we'll work on our 2014 dataset by making it more convenient to manipulate, and giving its time value an easier format to work on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87af91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2014 = df_avsep14.copy(deep=True)\n",
    "df_2014.rename(columns={key:str.lower(key) for key in df_2014.columns}, inplace=True)\n",
    "df_2014.sort_values(by='date/time', inplace=True)\n",
    "\n",
    "df_2014['date'] = df_2014['date/time'].str.split(\" \").str[0]\n",
    "df_2014['time'] = df_2014['date/time'].str.split(\" \").str[1]\n",
    "df_2014 = df_2014.drop('date/time', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d29da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2014['date'] = pd.to_datetime(df_2014['date'])\n",
    "df_2014['time'] = pd.to_datetime(df_2014['time']).dt.time\n",
    "df_2014['year'] = df_2014['date'].dt.year\n",
    "df_2014['month'] = df_2014['date'].dt.month\n",
    "df_2014['day'] = df_2014['date'].dt.day\n",
    "df_2014['dayofweek'] = df_2014['date'].dt.day_of_week\n",
    "\n",
    "df_2014.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f297631e",
   "metadata": {},
   "source": [
    "### 2015\n",
    "\n",
    "Then on our 2015 dataset, by making sense of its content before clarifying it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bf8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup = uber_dict['lookup']\n",
    "df_lookup.rename(columns={key:str.lower(key) for key in df_lookup.columns}, inplace=True)\n",
    "df_jajun15.rename(columns={key:str.lower(key) for key in df_jajun15.columns}, inplace=True)\n",
    "df_2015 = df_jajun15.merge(uber_dict['lookup'], on='locationid')\n",
    "df_2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb7b5d",
   "metadata": {},
   "source": [
    "Some pickup areas have a placeholder information \"Unknown\" which we'll leave blank instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea35ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015[['borough', 'zone']] = df_2015[['borough', 'zone']].replace('Unknown', '')\n",
    "print(df_2015['borough'].unique())\n",
    "print(sorted(df_2015['zone'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad2746a",
   "metadata": {},
   "source": [
    "##### GPS affiliation\n",
    "\n",
    "One thing sorely missing in our 2015 data are GPS coordinates, which we'll need for our work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf2275",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places = df_2015[[\"borough\", \"zone\"]].drop_duplicates()\n",
    "unique_places.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(timeout=10, user_agent=\"uber_app\")\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570d1940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(row):\n",
    "    try:\n",
    "        query = f\"USA, New York, {row['borough']}, {row['zone']}\"\n",
    "        location = geocode(query)\n",
    "        if location:\n",
    "            return pd.Series([location.latitude, location.longitude])\n",
    "        else:\n",
    "            try:\n",
    "                query = f\"USA, New York, {row['borough']}, {(row['zone'].split('/')[0])}\"\n",
    "                location = geocode(query)\n",
    "                if location:\n",
    "                    return pd.Series([location.latitude, location.longitude])\n",
    "                else:\n",
    "                    try:\n",
    "                        query = f\"USA, New York, {row['borough']}, {(row['zone'].split('/')[1])}\"\n",
    "                        location = geocode(query)\n",
    "                        if location:\n",
    "                            return pd.Series([location.latitude, location.longitude])\n",
    "                        else:\n",
    "                            return pd.Series([None, None])\n",
    "                    except:\n",
    "                        return pd.Series([None, None])\n",
    "            except:\n",
    "                return pd.Series([None, None])\n",
    "    except Exception:\n",
    "        return pd.Series([None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places[[\"lat\", \"lon\"]] = unique_places.apply(get_location, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e5abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015 = df_2015.merge(unique_places, on=[\"borough\", \"zone\"], how=\"left\")\n",
    "df_2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1a6d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b4bab",
   "metadata": {},
   "source": [
    "##### Missing values\n",
    "\n",
    "A large half of the values were missing without the second try/except in our geopy request. Here is a quick view of what we have in the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df759375",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94864d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015 = df_2015.dropna()\n",
    "df_2015.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a3d92",
   "metadata": {},
   "source": [
    "In the end, instead we have lost less than 10% of our data. We can thus keep the information that will match our 2014 dataset after the last transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bee909",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015 = df_2015.drop(columns=['borough', 'zone', 'locationid', 'affiliated_base_num'], axis = 1).reset_index()\n",
    "df_2015 = df_2015.rename(columns={'pickup_date': 'date/time', 'dispatching_base_num' : 'base' })\n",
    "df_2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed205df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015.sort_values(by='date/time', inplace=True)\n",
    "df_2015 = df_2015.drop(columns='index')\n",
    "\n",
    "df_2015['date'] = df_2015['date/time'].str.split(\" \").str[0]\n",
    "df_2015['time'] = df_2015['date/time'].str.split(\" \").str[1]\n",
    "df_2015 = df_2015.drop('date/time', axis=1)\n",
    "\n",
    "df_2015['date'] = pd.to_datetime(df_2015['date'])\n",
    "df_2015['time'] = pd.to_datetime(df_2015['time']).dt.time\n",
    "df_2015['year'] = df_2015['date'].dt.year\n",
    "df_2015['month'] = df_2015['date'].dt.month\n",
    "df_2015['day'] = df_2015['date'].dt.day\n",
    "df_2015['dayofweek'] = df_2015['date'].dt.day_of_week\n",
    "\n",
    "df_2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d461558",
   "metadata": {},
   "source": [
    "### Merging 2014+2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62754b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.concat([df_2014, df_2015], ignore_index=True)\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba81ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint creation. Careful with file size!\n",
    "#df_data.to_csv('../uber-trip-data/merge/uber_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c9783e",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "With our data now unified, we can start working on our clustering model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f4b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('../uber-trip-data/merge/uber_data.csv')\n",
    "df_data = df_data.drop('Unnamed: 0', axis=1)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f26f5dc",
   "metadata": {},
   "source": [
    "### Dissociating days\n",
    "\n",
    "Since we ultimately want to define hot zones per day, we will divide our data into dictionary entries depending on which day is it related to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7753a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_week = {\n",
    "    0 : 'Monday',\n",
    "    1 : 'Tuesday',\n",
    "    2 : 'Wednesday',\n",
    "    3 : 'Thursday',\n",
    "    4 : 'Friday',\n",
    "    5 : 'Saturday',\n",
    "    6 : 'Sunday'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7158a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_dict = {}\n",
    "day_dict_mini = {}\n",
    "\n",
    "for i in range(0,7):\n",
    "    day_dict[i] = df_data[df_data['dayofweek'] == i]\n",
    "    day_dict_mini[i] = day_dict[i].sample(10_000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda932b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_dict[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4cfa21",
   "metadata": {},
   "source": [
    "### MiniBatchKMEANS\n",
    "\n",
    "This is what we created our 10 000 sample for, to compare results with the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1674073b",
   "metadata": {},
   "source": [
    "##### Estimate daily clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca539c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_mkm(dayofweek):\n",
    "\n",
    "    sil = []\n",
    "    k = []\n",
    "    wcss =  []\n",
    "    \n",
    "    for i in range (2,21): \n",
    "        minikmeans = MiniBatchKMeans(n_clusters= i)\n",
    "        minikmeans.fit(day_dict_mini[dayofweek][['lat','lon']])\n",
    "        # elbow\n",
    "        wcss.append(minikmeans.inertia_)\n",
    "        \n",
    "        # sil score\n",
    "        sil.append(silhouette_score(day_dict_mini[dayofweek][['lat','lon']], minikmeans.predict(day_dict_mini[dayofweek][['lat','lon']])))\n",
    "        k.append(i)\n",
    "        # print(\"Silhouette score for K={} is {}\".format(i, sil[-1]))\n",
    "    # print(wcss)\n",
    "\n",
    "    # == elbow method ==\n",
    "    cluster_scores=pd.DataFrame(sil)\n",
    "    k_frame = pd.Series(k)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=k,\n",
    "        y=wcss, \n",
    "        mode='lines+markers', \n",
    "        name= 'WCSS', \n",
    "        yaxis='y1'\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=k,\n",
    "        y=sil, \n",
    "        name='sil_score',\n",
    "        opacity=0.3,\n",
    "        yaxis='y2'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"WCSS and Silhouette Score for {day_of_week[dayofweek]}\",\n",
    "        xaxis=dict(title=\"Number of Clusters (k)\"),\n",
    "        yaxis=dict(\n",
    "            title=\"WCSS\",\n",
    "            showgrid=False,\n",
    "            side=\"left\"\n",
    "        ),\n",
    "        yaxis2=dict(\n",
    "            title=\"Silhouette Score\",\n",
    "            overlaying=\"y\",\n",
    "            side=\"right\",\n",
    "            showgrid=False\n",
    "        ),\n",
    "        title_x=0.5,\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6508b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,7):\n",
    "    get_clusters_mkm(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4ec24f",
   "metadata": {},
   "source": [
    "##### Backup: MiniK scores\n",
    "![minik_sc_monday](./scores//minikmeans/minik_monday.png)\n",
    "![minik_sc_tuesday](./scores//minikmeans/minik_tuesday.png)\n",
    "![minik_sc_wednesday](./scores//minikmeans/minik_wednesday.png)\n",
    "![minik_sc_thursday](./scores//minikmeans/minik_thursday.png)\n",
    "![minik_sc_friday](./scores//minikmeans/minik_friday.png)\n",
    "![minik_sc_saturday](./scores//minikmeans/minik_saturday.png)\n",
    "![minik_sc_sunday](./scores//minikmeans/minik_sunday.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b042e7",
   "metadata": {},
   "source": [
    "##### Display clusters\n",
    "\n",
    "The difficulty in reading the graphs above is deciding which information is worth the most keeping: we want a balance between both scores. \n",
    "\n",
    "For business purposes, even though New York is a metropolis, it doesn't make much sense to dispatch drivers over 20 hot zones with several close to one another.\n",
    "\n",
    "We will keep the maximum number of clusters at most at 10. Thus we choose the following values:\n",
    "\n",
    "* Monday: 5 clusters\n",
    "* Tuesday: 6 clusters\n",
    "* Wednesday: 4 clusters\n",
    "* Thursday: 5 clusters\n",
    "* Friday: 6 clusters\n",
    "* Saturday: 5 clusters\n",
    "* Sunday: 5 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_mkm(dayofweek, cluster):\n",
    "    print('===== MiniKmeans =====')\n",
    "    print(f'show for {day_of_week[dayofweek]}')\n",
    "    minikmeans = MiniBatchKMeans(n_clusters=cluster, random_state=42)\n",
    "    minikmeans.fit(day_dict_mini[dayofweek][['lat', 'lon']])\n",
    "    day_dict_mini[dayofweek]['cluster'] = minikmeans.predict(day_dict_mini[dayofweek][['lat', 'lon']])\n",
    "\n",
    "    fig2 = px.scatter_map(day_dict_mini[dayofweek], \n",
    "                          lat='lat', \n",
    "                          lon='lon', \n",
    "                          color='cluster', \n",
    "                          color_continuous_scale='Bluered')\n",
    "    fig2.update_layout(\n",
    "        title = f\"All {cluster} clusters for {day_of_week[dayofweek]}\",\n",
    "        title_x=0.5\n",
    "        )\n",
    "    fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8447c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_mkm(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ada0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_mkm(1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_mkm(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_mkm(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6274cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_mkm(4,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6771933",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_mkm(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5bab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_mkm(6,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d105a95",
   "metadata": {},
   "source": [
    "##### Backup: MiniK plots\n",
    "![minik_5c_monday](./plots/minikmeans/minik_5c_monday.png)\n",
    "![minik_6c_tuesday](./plots/minikmeans/minik_6c_tuesday.png)\n",
    "![minik_4c_wednesday](./plots/minikmeans/minik_4c_wednesday.png)\n",
    "![minik_5c_thursday](./plots/minikmeans/minik_5c_thursday.png)\n",
    "![minik_6c_friday](./plots/minikmeans/minik_6c_friday.png)\n",
    "![minik_5c_saturday](./plots/minikmeans/minik_5c_saturday.png)\n",
    "![minik_5c_sunday](./plots/minikmeans/minik_5c_sunday.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f0cffb",
   "metadata": {},
   "source": [
    "##### Observations\n",
    "\n",
    "With the MiniBatch version of KMeans, we've been able to narrow down potential daily hot zones by covering vast areas of the city.\n",
    "\n",
    "Most of the time we can notice how Manhattan takes two of those, with a third around EWR airport.\n",
    "\n",
    "Although it seems to fit well the overall shape of the city, the model doesn't know what to do with geography, which would turn in real conditions into an issue: navigating around a metropolis under 10 minutes can prove difficult as a taxi driver if you must cross a bridge to reach your customer.\n",
    "\n",
    "Anyway, now how does the classic algorithm compare to the MiniBatch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a4a70",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa624ef",
   "metadata": {},
   "source": [
    "##### Estimate daily clusters\n",
    "\n",
    "We'll keep running our sample for the sake of computation time (10 000 rows against 17+ millions) as well as for the sake of having the same input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a4742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_km(dayofweek):\n",
    "\n",
    "    sil = []\n",
    "    k = []\n",
    "    wcss =  []\n",
    "    \n",
    "    for i in range (2,11): \n",
    "        kmeans = KMeans(n_clusters= i)\n",
    "        kmeans.fit(day_dict_mini[dayofweek][['lat','lon']])\n",
    "        # elbow\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        \n",
    "        # sil score\n",
    "        sil.append(silhouette_score(day_dict_mini[dayofweek][['lat','lon']], kmeans.predict(day_dict_mini[dayofweek][['lat','lon']])))\n",
    "        k.append(i)\n",
    "        # print(\"Silhouette score for K={} is {}\".format(i, sil[-1]))\n",
    "    # print(wcss)\n",
    "\n",
    "    # == elbow method ==\n",
    "    cluster_scores=pd.DataFrame(sil)\n",
    "    k_frame = pd.Series(k)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=k,\n",
    "        y=wcss, \n",
    "        mode='lines+markers', \n",
    "        name= 'WCSS', \n",
    "        yaxis='y1'\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=k,\n",
    "        y=sil, \n",
    "        name='sil_score',\n",
    "        opacity=0.3,\n",
    "        yaxis='y2'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"WCSS and Silhouette Score for {day_of_week[dayofweek]}\",\n",
    "        xaxis=dict(title=\"Number of Clusters (k)\"),\n",
    "        yaxis=dict(\n",
    "            title=\"WCSS\",\n",
    "            showgrid=False,\n",
    "            side=\"left\"\n",
    "        ),\n",
    "        yaxis2=dict(\n",
    "            title=\"Silhouette Score\",\n",
    "            overlaying=\"y\",\n",
    "            side=\"right\",\n",
    "            showgrid=False\n",
    "        ),\n",
    "        title_x=0.5,\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a4ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,7):\n",
    "    get_clusters_km(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ed18d2",
   "metadata": {},
   "source": [
    "##### Backup: KMeans scores\n",
    "![kmeans_sc_monday](./scores/kmeans/kmeans_monday.png)\n",
    "![kmeans_sc_tuesday](./scores/kmeans/kmeans_tuesday.png)\n",
    "![kmeans_sc_wednesday](./scores/kmeans/kmeans_wednesday.png)\n",
    "![kmeans_sc_thursday](./scores/kmeans/kmeans_thursday.png)\n",
    "![kmeans_sc_friday](./scores/kmeans/kmeans_friday.png)\n",
    "![kmeans_sc_saturday](./scores/kmeans/kmeans_saturday.png)\n",
    "![kmeans_sc_sunday](./scores/kmeans/kmeans_sunday.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd0a859",
   "metadata": {},
   "source": [
    "##### Display clusters\n",
    "\n",
    "* Monday: 5 clusters\n",
    "* Tuesday: 5 clusters\n",
    "* Wednesday: 5 clusters\n",
    "* Thursday: 4 clusters\n",
    "* Friday: 4 clusters\n",
    "* Saturday: 5 clusters\n",
    "* Sunday: 5 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_km(dayofweek, cluster):\n",
    "    print('===== Kmeans =====')\n",
    "    print(f'show for {day_of_week[dayofweek]}')\n",
    "    kmeans = KMeans(n_clusters=cluster, random_state=42)\n",
    "    kmeans.fit(day_dict_mini[dayofweek][['lat', 'lon']])\n",
    "    day_dict_mini[dayofweek]['cluster'] = kmeans.predict(day_dict_mini[dayofweek][['lat', 'lon']])\n",
    "\n",
    "    fig2 = px.scatter_map(day_dict_mini[dayofweek], \n",
    "                          lat='lat', \n",
    "                          lon='lon', \n",
    "                          color='cluster', \n",
    "                          color_continuous_scale='Bluered')\n",
    "    fig2.update_layout(\n",
    "        title = f\"All {cluster} clusters for {day_of_week[dayofweek]}\",\n",
    "        title_x=0.5\n",
    "        )\n",
    "    fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3701b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_km(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_km(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e631d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_km(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbd2ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_km(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a23eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_km(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_km(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5991e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_km(6,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b194586",
   "metadata": {},
   "source": [
    "##### Backup: KMeans plots\n",
    "![kmeans_5c_monday](./plots/kmeans/kmeans_5c_monday.png)\n",
    "![kmeans_5c_tuesday](./plots/kmeans/kmeans_5c_tuesday.png)\n",
    "![kmeans_5c_wednesday](./plots/kmeans/kmeans_5c_wednesday.png)\n",
    "![kmeans_4c_thursday](./plots/kmeans/kmeans_4c_thursday.png)\n",
    "![kmeans_4c_friday](./plots/kmeans/kmeans_4c_friday.png)\n",
    "![kmeans_5c_saturday](./plots/kmeans/kmeans_5c_saturday.png)\n",
    "![kmeans_5c_sunday](./plots/kmeans/kmeans_5c_sunday.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c19a110",
   "metadata": {},
   "source": [
    "##### Observations\n",
    "\n",
    "Although results slightly change, clustering trends remain the same between both algorithms - the same way the issues do.\n",
    "\n",
    "We've covered two versions of KMeans. Let's try a different algorithm now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71753aa",
   "metadata": {},
   "source": [
    "### DBScan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc854ad",
   "metadata": {},
   "source": [
    "##### Estimate daily clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf7d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_dbs(dayofweek):    \n",
    "    dbs_results = []\n",
    "\n",
    "    for eps in np.arange(0.005, 0.5, 0.05):\n",
    "        for min_samples in range(10, 1000, 100):\n",
    "            db = DBSCAN(eps=eps, min_samples=min_samples, metric=\"euclidean\")\n",
    "            labels = db.fit_predict(day_dict_mini[dayofweek][['lat', 'lon']])\n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "            # business purpose: filter to have number of clusters between 5 and 10\n",
    "            if 4 <= n_clusters <= 10:\n",
    "                day_dict_mini[dayofweek]['cluster'] = db.labels_\n",
    "                max_item = max(day_dict_mini[dayofweek]['cluster'].value_counts())\n",
    "                min_item = min(day_dict_mini[dayofweek]['cluster'].value_counts())\n",
    "\n",
    "                dbs_results.append({'eps': eps, 'min_samp' : min_samples, 'n_clusters': n_clusters, 'max_item' : max_item, 'min_item' : min_item})\n",
    "    \n",
    "    dbs_results = pd.DataFrame(dbs_results, columns=['eps', 'min_samp', 'n_clusters','max_item', 'min_item'])\n",
    "    # dbs_results = dbs_results.sort_values(by=\"max_item\", ascending=False)\n",
    "    # print(f'results for {day_of_week[dayofweek]}')\n",
    "    # print(dbs_results.head())\n",
    "    return dbs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dbs_day = {}\n",
    "\n",
    "for i in range (0,7):\n",
    "    results_dbs_day[i]= get_clusters_dbs(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7421c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,7):\n",
    "    print(fr\"The possible clusters on {day_of_week[i]} are: {results_dbs_day[i]['n_clusters'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,7):\n",
    "    results_dbs_day[i] = results_dbs_day[i].sort_values(by=\"max_item\", ascending=False)\n",
    "    print('='*50)\n",
    "    print(f'day {day_of_week[i]}')\n",
    "    print(results_dbs_day[i].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b15437",
   "metadata": {},
   "source": [
    "##### Backup: training output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e8d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==================================================\n",
    "day Monday\n",
    "     eps  min_samp  n_clusters  max_item  min_item\n",
    "0  0.005       110           5      6575       118\n",
    "3  0.005       410           4      6376       183\n",
    "1  0.005       210           7      5263        43\n",
    "2  0.005       310           6      4670       323\n",
    "==================================================\n",
    "day Tuesday\n",
    "     eps  min_samp  n_clusters  max_item  min_item\n",
    "0  0.005       110           4      6883       189\n",
    "2  0.005       410           4      5635       199\n",
    "1  0.005       210           6      5560        45\n",
    "==================================================\n",
    "day Wednesday\n",
    "     eps  min_samp  n_clusters  max_item  min_item\n",
    "0  0.005       110           5      7161       120\n",
    "2  0.005       410           4      5886       246\n",
    "1  0.005       210           5      5817        44\n",
    "==================================================\n",
    "day Thursday\n",
    "     eps  min_samp  n_clusters  max_item  min_item\n",
    "0  0.005       110           5      6900       154\n",
    "1  0.005       410           5      5241       371\n",
    "==================================================\n",
    "day Friday\n",
    "     eps  min_samp  n_clusters  max_item  min_item\n",
    "0  0.005       110           6      6655       127\n",
    "3  0.005       410           4      5933       450\n",
    "1  0.005       210           4      5753       267\n",
    "2  0.005       310           4      4662        99\n",
    "==================================================\n",
    "day Saturday\n",
    "     eps  min_samp  n_clusters  max_item  min_item\n",
    "0  0.005       110           5      6339       125\n",
    "3  0.005       410           6      5557       416\n",
    "1  0.005       210           4      5196       239\n",
    "2  0.005       310           4      5191        98\n",
    "==================================================\n",
    "day Sunday\n",
    "     eps  min_samp  n_clusters  max_item  min_item\n",
    "2  0.005       410           6      6126         7\n",
    "0  0.005       110           8      5990       112\n",
    "1  0.005       310           4      5378       332\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a676dcc",
   "metadata": {},
   "source": [
    "##### Display clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7702ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_dbs(dayofweek, eps_value, min_samp_value):\n",
    "    db = DBSCAN(eps=eps_value, min_samples=min_samp_value, metric=\"euclidean\")\n",
    "    db.fit(day_dict_mini[dayofweek][['lat', 'lon']])\n",
    "    day_dict_mini[dayofweek]['cluster'] = db.labels_\n",
    "    # labels = db.fit_predict(day_dict_mini[dayofweek][['lat', 'lon']])\n",
    "    n_clusters = day_dict_mini[dayofweek]['cluster'].value_counts().shape[0]\n",
    "    print('===== DBScan =====')\n",
    "    print(f'show for {day_of_week[dayofweek]}')\n",
    "    print(day_dict_mini[dayofweek]['cluster'].value_counts())\n",
    "\n",
    "    fig3 = px.scatter_map(day_dict_mini[dayofweek], \n",
    "                          lat='lat', \n",
    "                          lon='lon', \n",
    "                          color='cluster', \n",
    "                          color_continuous_scale='Bluered')\n",
    "    fig3.update_layout(\n",
    "        title = f\"All {n_clusters} clusters for {day_of_week[dayofweek]}\",\n",
    "        title_x=0.5\n",
    "        )\n",
    "    fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f6402",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_dbs(0,0.005,110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d5f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_dbs(1,0.005,110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2151569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_dbs(2,0.005,110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_dbs(3,0.005,110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2df3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_dbs(4,0.005,110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84f0a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_dbs(5,0.005,110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ab4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_dbs(6,0.005,110)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d2215",
   "metadata": {},
   "source": [
    "##### Backup: DBscan plots\n",
    "![dbscan_5c_monday](./plots/dbscan/dbscan_5c_monday.png)\n",
    "![dbscan_4c_tuesday](./plots/dbscan/dbscan_4c_tuesday.png)\n",
    "![dbscan_5c_wednesday](./plots/dbscan/dbscan_5c_wednesday.png)\n",
    "![dbscan_5c_thursday](./plots/dbscan/dbscan_5c_thursday.png)\n",
    "![dbscan_6c_friday](./plots/dbscan/dbscan_6c_friday.png)\n",
    "![dbscan_5c_saturday](./plots/dbscan/dbscan_5c_saturday.png)\n",
    "![dbscan_8c_sunday](./plots/dbscan/dbscan_8c_sunday.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1ef9d5",
   "metadata": {},
   "source": [
    "##### Observations\n",
    "\n",
    "While KMeans is really handy at designing areas, playing around with DBscan's settings lets us pinpoint zones with the most customer pick-up requests.\n",
    "\n",
    "While very efficient at that, and thus better suited at adapting to geography (such as crossing the Hudson river here), it can conversely lose itself in an especially dense zone such as Manhattan. This is best explained with the numbers we produced while working on this, with this last example of sunday:\n",
    "\n",
    "![DBscan_example](./plots/dbscan_example.jpg)\n",
    "\n",
    "These mean not only does Manhattan's cluster cover 60% of our pickups, but the remaining ones represent between 1 and 2%.\n",
    "\n",
    "This is the kind of extra and very needed information that would help dispatching drivers in proportion to the projected needs!\n",
    "\n",
    "Now it is largely possible to further optimize it by playing some more with DBscan's parameters; regardless for the time being, although not optimal because of geography, these points make for interesting locations for a driver to stand by until a customer requests transportation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1321c16",
   "metadata": {},
   "source": [
    "# Concluding\n",
    "\n",
    "Both algorithms, KMeans and DBscan, have their ups and downs. While one will definitely produce entire areas, the other can be made to pinpoint intensity.\n",
    "\n",
    "Although they can compliment one another, this project's objective is to produce *hot zones*. Quantity here is key and would give DBscan the edge.\n",
    "\n",
    "It would take a lot more testing to play around with its settings to possibly produce even more accurate locations for drivers to be in for quick response.\n",
    "\n",
    "DBscan's results can be otherwise more complex or diverse; we will provide an example of it below on sundays. For now we'll stick with this algorithm first, but we should still keep in mind KMeans to avoid forgetting about more distant customers whom could be made to wait well over 10 or 15 minutes!\n",
    "\n",
    "On sunday, 8 clusters versus 6:\n",
    "![dbscan_8c_sunday](./plots/dbscan/dbscan_8c_sunday.png)\n",
    "![dbscan_6c_sunday](./plots/dbscan_6c_sunday.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
